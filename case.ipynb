{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909ddc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff7705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/work/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5f66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# from test_transformers.src.test2_transformers.models.t5.modeling_t5 import T5ForConditionalGeneration\n",
    "# from test_transformers.src.test2_transformers.models.longt5.modeling_longt5 import LongT5ForConditionalGeneration\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1, 2\"\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88bedba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import data_process\n",
    "from inference import keyphrases_selection,calculate_score, get_score_df\n",
    "from torch.utils.data import DataLoader\n",
    "# from transformers import T5ForConditionalGeneration, LongT5ForConditionalGeneration\n",
    "\n",
    "\n",
    "def get_setting_dict():\n",
    "    setting_dict = {}\n",
    "    setting_dict[\"max_len\"] = 512\n",
    "    \n",
    "    # setting_dict[\"temp_en1\"] = \"Answer the following question by reasoning step-by-step. \"\n",
    "    setting_dict[\"temp_en2\"] = \"Keyword extraction: Extract keywords that reflect the key ideas of this book. \"\n",
    "\n",
    "    setting_dict[\"temp_en1\"] = \"Book:\"\n",
    "    setting_dict[\"temp_de1\"] = \"This book mainly talks about \"\n",
    "    # setting_dict[\"temp_de1\"] = \"Keyphrases: \"\n",
    "    setting_dict[\"temp_de2\"] = \"And in certain sections, this book talks about \"\n",
    "    setting_dict[\"temp_de3\"] = \"Therefore, the keyphrases of this text are \"\n",
    "    setting_dict[\"model\"] = \"base\"\n",
    "    setting_dict[\"enable_filter\"] = False\n",
    "    setting_dict[\"enable_pos\"] = True\n",
    "    setting_dict[\"enable_freq\"] = False\n",
    "    setting_dict[\"enable_att\"] = True\n",
    "    setting_dict[\"position_factor\"] = 1.2e8\n",
    "    setting_dict[\"length_factor\"] = 1.2\n",
    "    return setting_dict\n",
    "\n",
    "def parse_argument(dataset_dir=None, dataset_name=None, batch_size=None, log_dir=None):\n",
    "\n",
    "    if not all([dataset_dir, dataset_name, batch_size, log_dir]):\n",
    "        raise ValueError(\"All arguments (dataset_dir, dataset_name, batch_size, log_dir) must be provided.\")\n",
    "\n",
    "    import argparse\n",
    "    parser = argparse.Namespace(\n",
    "        dataset_dir=dataset_dir,\n",
    "        dataset_name=dataset_name,\n",
    "        batch_size=batch_size,\n",
    "        log_dir=log_dir\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724858b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"DUC2001\"\n",
    "args = parse_argument(\n",
    "    dataset_dir=f\"data/{dataset}\", \n",
    "    dataset_name=f\"{dataset}\",\n",
    "    batch_size=12,\n",
    "    log_dir=\"path/to/log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54bd2e",
   "metadata": {},
   "source": [
    "# 실험하자자자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf223cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ee49caeb054f98b6014c99a6feae7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c0c4414c6d42b48956f4513c78ddd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4583e87a49374a43baef4bd943df74da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cc6e017f3c4967880f84cb81e3b83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae367764d63e4f1db6412828c1b546d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/856 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:bitsandbytes.cextension:WARNING: BNB_CUDA_VERSION=123 environment variable detected; loading libbitsandbytes_cuda123.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd6a6e14d9c436ab2655e057a8a5391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a823d9ed91b54f8f8af055476e525bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23316b2d8e5144f487a955f48137e512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4163aaf24e43b9b90aa0f04189d176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be91938112844473ab77863f21f7e474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebcf43835a547b8939e07e0742c8d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61833428243e4b78adcfbeecc93d97ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f2c828951e407e880f4231bfa47abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bc4726ec114bddb03e6be6de92828b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5ccaee2eeb4110bff1bd017d2e3434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/2.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca75d0a8f364b459492f4a4d6d8d02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b537316595314d0fb871b3209bb2a58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n",
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_num:  35926\n",
      "examples:  35829\n"
     ]
    }
   ],
   "source": [
    "from transformers import GemmaTokenizer\n",
    "from transformers import Gemma2ForCausalLM\n",
    "# from test_transformers.src.test2_transformers.models.gemma2.modeling_gemma2_normal import Gemma2ForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "setting_dict = get_setting_dict()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = GemmaTokenizer.from_pretrained(\"google/gemma-2-9b\", padding_side = 'left')\n",
    "# tokenizer = GemmaTokenizer.from_pretrained(\n",
    "#     \"google/gemma-2-9b-it\",\n",
    "#     padding_side=\"left\",  # 생성 작업 필수\n",
    "#     truncation_side=\"left\",\n",
    "#     pad_token=\"<pad>\",  # 명시적 패딩 토큰 지정\n",
    "#     add_eos_token=True  # 문장 종료 신호 강화\n",
    "# )\n",
    "\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16  # 계산 속도 향상\n",
    "# )\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = Gemma2ForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    quantization_config=quant_config,\n",
    "    # device_map=\"auto\",\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    output_attentions=True \n",
    ")\n",
    "\n",
    "dataset, doc_list, labels, labels_stemed, docs_candidates, att_candidates, cand_sim_group = data_process(setting_dict, args.dataset_dir, args.dataset_name, 23,1, model, device)\n",
    "dataloader = DataLoader(dataset, num_workers=12, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d56846d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0af10d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the weighted gemma2 version!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.default/hyeongu/PromptRank_cross_att_gemma2/test_transformers/src/test2_transformers/generation/configuration_utils.py:816: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c41d4ed6f649a39d1a6f521fa9548c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from test_transformers.src.test2_transformers.models.gemma2.modeling_gemma2_normal import Gemma2ForCausalLM_our\n",
    "\n",
    "model2 = Gemma2ForCausalLM_our.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    quantization_config=quant_config,\n",
    "#     device_map=\"auto\",\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    output_attentions=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3b3ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of transformers failed: Traceback (most recent call last):\n",
      "  File \"/home/work/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/work/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/work/.default/anaconda3/envs/hyeongu_base/lib/python3.10/importlib/__init__.py\", line 139, in reload\n",
      "    name = module.__spec__.name\n",
      "  File \"/home/work/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1789, in __getattr__\n",
      "    if name in self._objects:\n",
      "  File \"/home/work/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1789, in __getattr__\n",
      "    if name in self._objects:\n",
      "  File \"/home/work/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 1789, in __getattr__\n",
      "    if name in self._objects:\n",
      "  [Previous line repeated 4 more times]\n",
      "RecursionError: maximum recursion depth exceeded while calling a Python object\n",
      "]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Evaluating::   0%|          | 0/2986 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Evaluating::   0%|          | 1/2986 [00:04<3:28:58,  4.20s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 401.94 MiB is free. Process 2245 has 8.00 GiB memory in use. Process 791524 has 18.04 GiB memory in use. Including non-PyTorch memory, this process has 52.74 GiB memory in use. Of the allocated memory 50.54 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cosine_similarity_rank, pred_labels \u001b[38;5;241m=\u001b[39m \u001b[43mkeyphrases_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_stemed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# cosine_similarity_rank, ori_pred_labels =keyphrases_selection_original(setting_dict, doc_list, labels_stemed, labels,  model, dataloader, device)\u001b[39;00m\n",
      "File \u001b[0;32m~/.default/hyeongu/PromptRank_cross_att_gemma2/inference.py:99\u001b[0m, in \u001b[0;36mkeyphrases_selection\u001b[0;34m(setting_dict, doc_list, labels_stemed, labels, model, dataloader, initial_std, std_layer_product, topk, temp, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcand_pos_s\u001b[39m\u001b[38;5;124m\"\u001b[39m: dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5_pos_s\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcand_pos_e\u001b[39m\u001b[38;5;124m\"\u001b[39m: dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5_pos_e\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msim_word_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msim_word_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     96\u001b[0m }\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 99\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# (B, L, V)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.default/hyeongu/PromptRank_cross_att_gemma2/test_transformers/src/test2_transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.default/hyeongu/PromptRank_cross_att_gemma2/test_transformers/src/test2_transformers/models/gemma2/modeling_gemma2_normal.py:1056\u001b[0m, in \u001b[0;36mGemma2ForCausalLM_our.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1056\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# 디코더 연산 수행, 결과 반환\u001b[39;49;00m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#output_hidden_states,\u001b[39;49;00m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# hidden_states = outputs[0]   \u001b[39;00m\n\u001b[1;32m   1071\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]       \n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.default/hyeongu/PromptRank_cross_att_gemma2/test_transformers/src/test2_transformers/models/gemma2/modeling_gemma2_normal.py:839\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    828\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    829\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    836\u001b[0m         cache_position,\n\u001b[1;32m    837\u001b[0m     )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 839\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.default/hyeongu/PromptRank_cross_att_gemma2/test_transformers/src/test2_transformers/models/gemma2/modeling_gemma2_normal.py:505\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    517\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.default/hyeongu/PromptRank_cross_att_gemma2/test_transformers/src/test2_transformers/models/gemma2/modeling_gemma2_normal.py:440\u001b[0m, in \u001b[0;36mGemma2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m combined_weight\u001b[38;5;241m=\u001b[39m apply_custom_cross_attention_weighting(\n\u001b[1;32m    418\u001b[0m     query_length\u001b[38;5;241m=\u001b[39mquery_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    419\u001b[0m     key_length\u001b[38;5;241m=\u001b[39mkey_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     mix_topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \n\u001b[1;32m    427\u001b[0m )\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# padding_mask = (attention_mask < -1e2)   # shape: [B, 1, 1, K]\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# weights = torch.where(padding_mask, torch.ones_like(weights), gaussian_weights)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m#########################################################\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftcap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_logit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgaussian_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()  \u001b[38;5;66;03m#멀티헤드 결과를 다시 하나로 합치는 것입니다. (concat)\u001b[39;00m\n\u001b[1;32m    455\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/.default/hyeongu/PromptRank_cross_att_gemma2/test_transformers/src/test2_transformers/models/gemma2/modeling_gemma2_normal.py:220\u001b[0m, in \u001b[0;36meager_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, softcap, gaussian_weights, layer_idx, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m mask \u001b[38;5;241m=\u001b[39m (gaussian_weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# gaussian_weight가 1인 곳은 attn_weights 유지, 나머지만 조정\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# adjusted_weights = adjusted_weights / temperature\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m*\u001b[39m gaussian_weights \n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# attn_weights = nn.functional.softmax(adjusted_weights, dim=-1, dtype=torch.float32).to(query.dtype)\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m###########\u001b[39;00m\n\u001b[1;32m    224\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m (attn_weights\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "File \u001b[0;32m~/.default/anaconda3/envs/hyeongu_base/lib/python3.10/site-packages/torch/nn/functional.py:2142\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 401.94 MiB is free. Process 2245 has 8.00 GiB memory in use. Process 791524 has 18.04 GiB memory in use. Including non-PyTorch memory, this process has 52.74 GiB memory in use. Of the allocated memory 50.54 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "cosine_similarity_rank, pred_labels = keyphrases_selection(setting_dict, doc_list, labels_stemed, labels, model2, dataloader,0.3,0.4,15,1, device)\n",
    "# cosine_similarity_rank, ori_pred_labels =keyphrases_selection_original(setting_dict, doc_list, labels_stemed, labels,  model, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82178d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3281872509960159 0.3696357735304724 0.3594104308390023\n"
     ]
    }
   ],
   "source": [
    "a,df2=calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,0,1,1,0,0.7,1.2e8,0.6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e325f1f-d060-42eb-8ae7-fab0704985b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,df2=calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,0,1,1,0,0.7,1.2e8,0.6,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48614bf1-feb3-4c45-9932-83968269df7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f5': 32.82, 'f10': 36.96, 'f15': 35.94}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40a07e6b-180c-4d80-a4e3-df2b07678eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('duc_our_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f68efcd6-dc10-4671-88b2-ae65f34979a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>candidate</th>\n",
       "      <th>score</th>\n",
       "      <th>pos</th>\n",
       "      <th>att_score</th>\n",
       "      <th>candidate_len</th>\n",
       "      <th>whole_att_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11245</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>abimael guzman</td>\n",
       "      <td>tensor(-2.9557)</td>\n",
       "      <td>tensor(75)</td>\n",
       "      <td>tensor(0.5713, dtype=torch.float64)</td>\n",
       "      <td>tensor(5)</td>\n",
       "      <td>tensor(0.9258, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11246</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>10-point agreement</td>\n",
       "      <td>tensor(-3.4281)</td>\n",
       "      <td>tensor(138)</td>\n",
       "      <td>tensor(0.5757, dtype=torch.float64)</td>\n",
       "      <td>tensor(6)</td>\n",
       "      <td>tensor(0.6602, dtype=torch.float64)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11247</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>peace agreement</td>\n",
       "      <td>tensor(-3.4686)</td>\n",
       "      <td>tensor(44)</td>\n",
       "      <td>tensor(1.1904, dtype=torch.float64)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(1.1250, dtype=torch.float64)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11248</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>president alberto fujimori</td>\n",
       "      <td>tensor(-3.7336)</td>\n",
       "      <td>tensor(65)</td>\n",
       "      <td>tensor(1.0125, dtype=torch.float64)</td>\n",
       "      <td>tensor(5)</td>\n",
       "      <td>tensor(0.7930, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11249</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>tensor(-4.2999)</td>\n",
       "      <td>tensor(158)</td>\n",
       "      <td>tensor(1.2139, dtype=torch.float64)</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>tensor(1.5547, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11250</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>war</td>\n",
       "      <td>tensor(-4.5909)</td>\n",
       "      <td>tensor(237)</td>\n",
       "      <td>tensor(1.0017, dtype=torch.float64)</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>tensor(0.7266, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11251</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>peruvian government</td>\n",
       "      <td>tensor(-4.6402)</td>\n",
       "      <td>tensor(59)</td>\n",
       "      <td>tensor(0.8962, dtype=torch.float64)</td>\n",
       "      <td>tensor(3)</td>\n",
       "      <td>tensor(1.1250, dtype=torch.float64)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11252</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>popular war</td>\n",
       "      <td>tensor(-5.0873)</td>\n",
       "      <td>tensor(149)</td>\n",
       "      <td>tensor(0.8358, dtype=torch.float64)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(1.4453, dtype=torch.float64)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>agreement</td>\n",
       "      <td>tensor(-5.1082)</td>\n",
       "      <td>tensor(381)</td>\n",
       "      <td>tensor(0.8342, dtype=torch.float64)</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>tensor(0.1777, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11254</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>general amnesty</td>\n",
       "      <td>tensor(-5.2777)</td>\n",
       "      <td>tensor(216)</td>\n",
       "      <td>tensor(2.3247, dtype=torch.float64)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(1.0156, dtype=torch.float64)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11255</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>self-dismantling</td>\n",
       "      <td>tensor(-5.3281)</td>\n",
       "      <td>tensor(198)</td>\n",
       "      <td>tensor(1.4955, dtype=torch.float64)</td>\n",
       "      <td>tensor(5)</td>\n",
       "      <td>tensor(0.4746, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11256</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>political prisoners</td>\n",
       "      <td>tensor(-5.6858)</td>\n",
       "      <td>tensor(239)</td>\n",
       "      <td>tensor(1.1434, dtype=torch.float64)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(0.4785, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11257</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>government authorities</td>\n",
       "      <td>tensor(-5.6858)</td>\n",
       "      <td>tensor(370)</td>\n",
       "      <td>tensor(1.4212, dtype=torch.float64)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(0.2852, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11258</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>economic support</td>\n",
       "      <td>tensor(-5.7402)</td>\n",
       "      <td>tensor(354)</td>\n",
       "      <td>tensor(1.2911, dtype=torch.float64)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(0.3516, dtype=torch.float64)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11259</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>government</td>\n",
       "      <td>tensor(-5.8518)</td>\n",
       "      <td>tensor(384)</td>\n",
       "      <td>tensor(1.2667, dtype=torch.float64)</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>tensor(0.1572, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11260</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>government representatives</td>\n",
       "      <td>tensor(-5.9034)</td>\n",
       "      <td>tensor(92)</td>\n",
       "      <td>tensor(0.8073, dtype=torch.float64)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(0.7109, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11261</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>talks</td>\n",
       "      <td>tensor(-5.9487)</td>\n",
       "      <td>tensor(71)</td>\n",
       "      <td>tensor(0.8517, dtype=torch.float64)</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>tensor(1.5391, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>people</td>\n",
       "      <td>tensor(-5.9811)</td>\n",
       "      <td>tensor(179)</td>\n",
       "      <td>tensor(0.6892, dtype=torch.float64)</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>tensor(0.6836, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11263</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>life sentence</td>\n",
       "      <td>tensor(-6.1211)</td>\n",
       "      <td>tensor(82)</td>\n",
       "      <td>tensor(0.5131, dtype=torch.float64)</td>\n",
       "      <td>tensor(2)</td>\n",
       "      <td>tensor(1.0547, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11264</th>\n",
       "      <td>tensor(125)</td>\n",
       "      <td>fighting</td>\n",
       "      <td>tensor(-6.4014)</td>\n",
       "      <td>tensor(156)</td>\n",
       "      <td>tensor(0.4960, dtype=torch.float64)</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>tensor(0.8359, dtype=torch.float64)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            doc_id                   candidate            score          pos  \\\n",
       "11245  tensor(125)              abimael guzman  tensor(-2.9557)   tensor(75)   \n",
       "11246  tensor(125)          10-point agreement  tensor(-3.4281)  tensor(138)   \n",
       "11247  tensor(125)             peace agreement  tensor(-3.4686)   tensor(44)   \n",
       "11248  tensor(125)  president alberto fujimori  tensor(-3.7336)   tensor(65)   \n",
       "11249  tensor(125)                   terrorism  tensor(-4.2999)  tensor(158)   \n",
       "11250  tensor(125)                         war  tensor(-4.5909)  tensor(237)   \n",
       "11251  tensor(125)         peruvian government  tensor(-4.6402)   tensor(59)   \n",
       "11252  tensor(125)                 popular war  tensor(-5.0873)  tensor(149)   \n",
       "11253  tensor(125)                   agreement  tensor(-5.1082)  tensor(381)   \n",
       "11254  tensor(125)             general amnesty  tensor(-5.2777)  tensor(216)   \n",
       "11255  tensor(125)            self-dismantling  tensor(-5.3281)  tensor(198)   \n",
       "11256  tensor(125)         political prisoners  tensor(-5.6858)  tensor(239)   \n",
       "11257  tensor(125)      government authorities  tensor(-5.6858)  tensor(370)   \n",
       "11258  tensor(125)            economic support  tensor(-5.7402)  tensor(354)   \n",
       "11259  tensor(125)                  government  tensor(-5.8518)  tensor(384)   \n",
       "11260  tensor(125)  government representatives  tensor(-5.9034)   tensor(92)   \n",
       "11261  tensor(125)                       talks  tensor(-5.9487)   tensor(71)   \n",
       "11262  tensor(125)                      people  tensor(-5.9811)  tensor(179)   \n",
       "11263  tensor(125)               life sentence  tensor(-6.1211)   tensor(82)   \n",
       "11264  tensor(125)                    fighting  tensor(-6.4014)  tensor(156)   \n",
       "\n",
       "                                 att_score candidate_len  \\\n",
       "11245  tensor(0.5713, dtype=torch.float64)     tensor(5)   \n",
       "11246  tensor(0.5757, dtype=torch.float64)     tensor(6)   \n",
       "11247  tensor(1.1904, dtype=torch.float64)     tensor(2)   \n",
       "11248  tensor(1.0125, dtype=torch.float64)     tensor(5)   \n",
       "11249  tensor(1.2139, dtype=torch.float64)     tensor(1)   \n",
       "11250  tensor(1.0017, dtype=torch.float64)     tensor(1)   \n",
       "11251  tensor(0.8962, dtype=torch.float64)     tensor(3)   \n",
       "11252  tensor(0.8358, dtype=torch.float64)     tensor(2)   \n",
       "11253  tensor(0.8342, dtype=torch.float64)     tensor(1)   \n",
       "11254  tensor(2.3247, dtype=torch.float64)     tensor(2)   \n",
       "11255  tensor(1.4955, dtype=torch.float64)     tensor(5)   \n",
       "11256  tensor(1.1434, dtype=torch.float64)     tensor(2)   \n",
       "11257  tensor(1.4212, dtype=torch.float64)     tensor(2)   \n",
       "11258  tensor(1.2911, dtype=torch.float64)     tensor(2)   \n",
       "11259  tensor(1.2667, dtype=torch.float64)     tensor(1)   \n",
       "11260  tensor(0.8073, dtype=torch.float64)     tensor(2)   \n",
       "11261  tensor(0.8517, dtype=torch.float64)     tensor(1)   \n",
       "11262  tensor(0.6892, dtype=torch.float64)     tensor(1)   \n",
       "11263  tensor(0.5131, dtype=torch.float64)     tensor(2)   \n",
       "11264  tensor(0.4960, dtype=torch.float64)     tensor(1)   \n",
       "\n",
       "                           whole_att_score  label  \n",
       "11245  tensor(0.9258, dtype=torch.float64)      0  \n",
       "11246  tensor(0.6602, dtype=torch.float64)      1  \n",
       "11247  tensor(1.1250, dtype=torch.float64)      1  \n",
       "11248  tensor(0.7930, dtype=torch.float64)      0  \n",
       "11249  tensor(1.5547, dtype=torch.float64)      0  \n",
       "11250  tensor(0.7266, dtype=torch.float64)      0  \n",
       "11251  tensor(1.1250, dtype=torch.float64)      1  \n",
       "11252  tensor(1.4453, dtype=torch.float64)      1  \n",
       "11253  tensor(0.1777, dtype=torch.float64)      0  \n",
       "11254  tensor(1.0156, dtype=torch.float64)      1  \n",
       "11255  tensor(0.4746, dtype=torch.float64)      0  \n",
       "11256  tensor(0.4785, dtype=torch.float64)      0  \n",
       "11257  tensor(0.2852, dtype=torch.float64)      0  \n",
       "11258  tensor(0.3516, dtype=torch.float64)      1  \n",
       "11259  tensor(0.1572, dtype=torch.float64)      0  \n",
       "11260  tensor(0.7109, dtype=torch.float64)      0  \n",
       "11261  tensor(1.5391, dtype=torch.float64)      0  \n",
       "11262  tensor(0.6836, dtype=torch.float64)      0  \n",
       "11263  tensor(1.0547, dtype=torch.float64)      0  \n",
       "11264  tensor(0.8359, dtype=torch.float64)      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['doc_id']==125][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909bbd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.17\n",
      "23.06\n",
      "22.89\n"
     ]
    }
   ],
   "source": [
    "a=calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,0,0.6,1.2e8,0.7,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40916767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adb7d3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_layer\n",
      "0.0 0.6\n",
      "local\n",
      "21.39\n",
      "24.20\n",
      "23.57\n",
      "20.88\n",
      "24.15\n",
      "23.85\n",
      "global\n",
      "21.10\n",
      "24.33\n",
      "24.39\n",
      "19.40\n",
      "22.88\n",
      "22.50\n",
      "local+global\n",
      "20.93\n",
      "24.50\n",
      "24.17\n",
      "19.40\n",
      "23.36\n",
      "22.86\n",
      "0.0 0.7\n",
      "local\n",
      "21.62\n",
      "24.20\n",
      "23.46\n",
      "20.65\n",
      "24.28\n",
      "23.96\n",
      "global\n",
      "20.88\n",
      "24.33\n",
      "24.14\n",
      "19.45\n",
      "22.45\n",
      "22.29\n",
      "local+global\n",
      "20.93\n",
      "24.50\n",
      "23.92\n",
      "19.11\n",
      "23.23\n",
      "22.68\n"
     ]
    }
   ],
   "source": [
    "initial = [0.]\n",
    "scaling=[0.6,0.7]\n",
    "print(\"att_layer\")\n",
    "for i in range(1):\n",
    "    for j in range(2):\n",
    "        print(initial[i], scaling[j])\n",
    "        print('local')\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,0,1,1,initial[i],scaling[j],1.2e8,0.6,0)\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,initial[i],scaling[j],1.2e8,0.6,0)\n",
    "        print('global')\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,0,1,1,initial[i],scaling[j],1.2e8,0.6,1)\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,initial[i],scaling[j],1.2e8,0.6,1)\n",
    "        print('local+global')\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,0,1,1,initial[i],scaling[j],1.2e8,0.6,2)\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,initial[i],scaling[j],1.2e8,0.6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec0805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_layer\n",
      "0.0 0.6\n",
      "local\n",
      "21.73\n",
      "24.15\n",
      "23.57\n",
      "20.53\n",
      "24.20\n",
      "23.78\n",
      "global\n",
      "21.56\n",
      "24.68\n",
      "24.35\n",
      "20.19\n",
      "23.06\n",
      "23.18\n",
      "local+global\n",
      "21.79\n",
      "24.41\n",
      "24.42\n",
      "20.25\n",
      "23.67\n",
      "23.28\n",
      "0.0 0.7\n",
      "local\n",
      "21.62\n",
      "24.20\n",
      "23.39\n",
      "20.48\n",
      "24.15\n",
      "23.68\n",
      "global\n",
      "21.56\n",
      "24.98\n",
      "24.24\n",
      "20.36\n",
      "22.97\n",
      "23.11\n",
      "local+global\n",
      "21.73\n",
      "24.55\n",
      "24.32\n",
      "20.25\n",
      "23.45\n",
      "23.28\n"
     ]
    }
   ],
   "source": [
    "initial = [0.]\n",
    "scaling=[0.6,0.7]\n",
    "print(\"att_layer\")\n",
    "for i in range(1):\n",
    "    for j in range(2):\n",
    "        print(initial[i], scaling[j])\n",
    "        print('local')\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,0,1,1,initial[i],scaling[j],1.2e8,0.6,0)\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,initial[i],scaling[j],1.2e8,0.6,0)\n",
    "        print('global')\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,0,1,1,initial[i],scaling[j],1.2e8,0.6,1)\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,initial[i],scaling[j],1.2e8,0.6,1)\n",
    "        print('local+global')\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,0,1,1,initial[i],scaling[j],1.2e8,0.6,2)\n",
    "        calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,initial[i],scaling[j],1.2e8,0.6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b3df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.84\n",
      "21.30\n",
      "21.55\n"
     ]
    }
   ],
   "source": [
    "aa=calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,0.,1,1.2e8,0.,2) # +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05947738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.57\n",
      "36.64\n",
      "35.54\n"
     ]
    }
   ],
   "source": [
    "aa=calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,0.3,0.6,1.2e8,0.6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bf4a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.52\n",
      "36.35\n",
      "35.54\n"
     ]
    }
   ],
   "source": [
    "aa=calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,0.,0.5,1.2e8,0.6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f323e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.17\n",
      "36.35\n",
      "35.69\n"
     ]
    }
   ],
   "source": [
    "aa=calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,0.,0.3,1.2e8,0.6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c6957e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.63\n",
      "24.17\n",
      "23.94\n"
     ]
    }
   ],
   "source": [
    "ranked_keyphrases= calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,0.2,0.4,1.2e8,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd561af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.83\n",
      "23.93\n",
      "23.81\n"
     ]
    }
   ],
   "source": [
    "ranked_keyphrases= calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,0.3,0.4,1.2e8,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d3df8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.13\n",
      "35.02\n",
      "34.44\n"
     ]
    }
   ],
   "source": [
    "ranked_keyphrases= calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,1,1,0.6,0.6,1.2e8,0.6) #1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d78355",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ranked_keyphrases\u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_score\u001b[49m(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0.6\u001b[39m,\u001b[38;5;241m0.4\u001b[39m,\u001b[38;5;241m1.2e8\u001b[39m,\u001b[38;5;241m0.6\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_score' is not defined"
     ]
    }
   ],
   "source": [
    "ranked_keyphrases= calculate_score(setting_dict, cosine_similarity_rank,doc_list,labels,labels_stemed,1,0,1,0.6,0.4,1.2e8,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de557595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20029895366218237\n",
      "0.24172317510969288\n",
      "0.23611572996341867\n"
     ]
    }
   ],
   "source": [
    "ranked_keyphrases= calculate_score(setting_dict, ori_cosine_similarity_rank,doc_list,labels,labels_stemed,0,0,1,0.6,0.4,1.2e8,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6214ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1993024414549078\n",
      "0.240127642600718\n",
      "0.23811107416029265\n"
     ]
    }
   ],
   "source": [
    "ranked_keyphrases= calculate_score(setting_dict, ori_cosine_similarity_rank,doc_list,labels,labels_stemed,1,0,1,0.6,0.4,1.2e8,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6894d308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1993024414549078\n",
      "0.24092540885520544\n",
      "0.23611572996341867\n"
     ]
    }
   ],
   "source": [
    "ranked_keyphrases= calculate_score(setting_dict, ori_cosine_similarity_rank,doc_list,labels,labels_stemed,0,0,1,0.6,0.4,1.2e8,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9bfadd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1983059292476333\n",
      "0.24092540885520544\n",
      "0.23877618889258398\n"
     ]
    }
   ],
   "source": [
    "ranked_keyphrases= calculate_score(setting_dict, ori_cosine_similarity_rank,doc_list,labels,labels_stemed,1,0,1,0.6,0.4,1.2e8,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_doc_pairs(doc, candidates, idx):\n",
    "    count = 0\n",
    "    doc_pairs = []\n",
    "    # task_prompt = \"Extract keyphrases from the text. The answer should be listed after 'Keyphrase: '.\"\n",
    "    task_prompt = \"Extract keyphrases from the text. The answer should be listed after 'This text mainly talks about '.\"\n",
    "    doc_prompt = f\"Text: {doc}\"\n",
    "   \n",
    "    # prompt = task_prompt + \" \" + doc_prompt\n",
    "    prompt = doc_prompt\n",
    "    for id, can_and_pos in enumerate(candidates):\n",
    "        candidate = can_and_pos\n",
    "        target_temp = \"Answer: This text mainly talks about \"\n",
    "        target = f\"Answer: This text mainly talks about {candidate}.\"\n",
    "        full_input = prompt + \" \" + target \n",
    "        input_ids = tokenizer(full_input, max_length=782, padding=\"max_length\", truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "        reconstructed_text = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        print(reconstructed_text)\n",
    "        target_temp_ids = tokenizer(target_temp, return_tensors=\"pt\")['input_ids'][0]\n",
    "        target_temp_len = target_temp_ids.size(0)\n",
    "        target_ids = tokenizer(target, return_tensors=\"pt\")['input_ids'][0]\n",
    "        target_len = target_ids.size(0)  # 맨 앞 '2' 토큰도 포함함\n",
    "        non_pad_len = (input_ids != tokenizer.pad_token_id).sum().item() \n",
    "        context_len = non_pad_len - target_len            # 딱 Answer 토큰 앞까지만 idx 44\n",
    "        start_idx = context_len + target_temp_len -1          # 딱 about 위치치     근데 inference에서 -1 더 해서 talks 위치로 했어\n",
    "        candidate_len= target_len - target_temp_len\n",
    "        \n",
    "\n",
    "        print(target_len)\n",
    "        # Find where the candidate tokens appear in the document\n",
    "#        en_input_id_list = en_input_ids[0].tolist()\n",
    "        candidate_position = None\n",
    "\n",
    "\n",
    "        dic = {\"candidate\":candidate, \"idx\":target_len, \"start_idx\":start_idx, 'cand_len':candidate_len, \"context_len\":context_len} \n",
    "        \n",
    "        doc_pairs.append([input_ids, dic])\n",
    "        # print(tokenizer.decode(en_input_ids[0]))\n",
    "        # print(tokenizer.decode(de_input_ids[0]))\n",
    "        # print(candidate)\n",
    "        # print(de_input_len)\n",
    "        # print()\n",
    "        # exit(0)\n",
    "\n",
    "    return doc_pairs, count, target_ids, start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d17ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9231)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "    logits = output.logits  # (B, L, V)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    score = torch.zeros(input_ids.size(0))\n",
    "\n",
    "    # 1. start_idx로 그룹 묶기\n",
    "    group_by_start = defaultdict(list)\n",
    "    for j in range(input_ids.size(0)):\n",
    "        group_by_start[dic[\"start_idx\"][j]].append(j)\n",
    "\n",
    "    for start, indices in group_by_start.items():\n",
    "        for j in indices:\n",
    "            cand_len = dic[\"candidate_len\"][j]\n",
    "            # 예측 위치: start ~ start + cand_len - 1\n",
    "            pos_indices = torch.arange(start, start + cand_len)\n",
    "            token_ids = input_ids[j, pos_indices + 1]  # 다음 위치에 등장해야 할 정답 토큰\n",
    "\n",
    "            log_probs_j = log_probs[j, pos_indices, :]  # (cand_len, V)\n",
    "            token_scores = log_probs_j.gather(1, token_ids.unsqueeze(1)).squeeze(1)  # (cand_len,)\n",
    "\n",
    "            score_j = token_scores.sum() / (cand_len ** length_factor)\n",
    "            score[j] = score_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b184f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "    logits = output.logits  # (B, L, V)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    score = torch.zeros(input_ids.size(0), device=input_ids.device)\n",
    "\n",
    "    # start_idx 기준으로 묶기\n",
    "    grouped = defaultdict(list)\n",
    "    for j in range(input_ids.size(0)):\n",
    "        grouped[dic[\"start_idx\"][j]].append(j)\n",
    "\n",
    "    for start, batch_indices in grouped.items():\n",
    "        for j in batch_indices:\n",
    "            cand_len = dic[\"candidate_len\"][j]\n",
    "            pos = torch.arange(start, start + cand_len, device=input_ids.device)\n",
    "            # 정답 토큰 ID (next token)\n",
    "            token_ids = input_ids[j, pos + 1]\n",
    "            log_p = log_probs[j, pos, :]  # shape: (cand_len, vocab)\n",
    "            selected_log_p = log_p.gather(1, token_ids.unsqueeze(1)).squeeze(1)  # shape: (cand_len,)\n",
    "            score[j] = selected_log_p.sum() / (cand_len ** length_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2de39dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028898854010961633\n",
      "0.04946150777822098\n",
      "0.07781842367808448\n"
     ]
    }
   ],
   "source": [
    "calculate_score(setting_dict, ori_cosine_similarity_rank,doc_list,labels,labels_stemed,1,0,0,0.6,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223315e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=get_score_df(setting_dict,cosine_similarity_rank,doc_list,labels,labels_stemed,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4a2329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(att_candidates[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf482a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv(\"result/wikihow_result_weight_0.5_0.5_not_pos.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset='inspec'\n",
    "file_path1 = f'{dataset}_pred_labels_ori.pkl'\n",
    "\n",
    "with open(file_path1, 'rb') as file:\n",
    "    ori = pickle.load(file)\n",
    "    \n",
    "file_path2 = f'{dataset}_pred_labels_1_0.5.pkl'\n",
    "\n",
    "\n",
    "with open(file_path2, 'rb') as file:\n",
    "    cross = pickle.load(file)\n",
    "    \n",
    "differences = {}\n",
    "\n",
    "for idx in ori:\n",
    "    ori_temp = set(ori[idx]['temp'])\n",
    "    cross_temp = set(cross[idx]['temp'])\n",
    "    \n",
    "    # 차이점 계산\n",
    "    diff_ori = ori_temp - cross_temp  # ori에서만 있는 값\n",
    "    diff_cross = cross_temp - ori_temp  # cross에서만 있는 값\n",
    "    \n",
    "    differences[idx] = {'ori_only': list(diff_ori), 'cross_only': list(diff_cross)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyeongu_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
